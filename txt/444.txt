In the wake of the 2016 presidential election, as online platforms began facing greater scrutiny for their impacts on users, elections and society, many tech firms started investing in safeguards. Big Tech companies brought on employees focused on election safety, misinformation and online extremism. Some also formed ethical AI teams and invested in oversight groups. These teams helped guide new safety features and policies. But over the past few months, large tech companies have slashed tens of thousands of jobs, and some of those same teams are seeing staff reductions. Twitter eliminated teams focused on security, public policy and human rights issues when Elon Musk took over last year. More recently, Twitch, a livestreaming platform owned by Amazon, laid off some employees focused on responsible AI and other trust and safety work, according to former employees and public social media posts. Microsoft cut a key team focused on ethical AI product development. And Facebook-parent Meta suggested that it might cut staff working in non-technical roles as part of its latest round of layoffs. Meta, according to CEO Mark Zuckerberg, hired "many leading experts in areas outside engineering." Now, he said, the company will aim to return "to a more optimal ratio of engineers to other roles," as part of cuts set to take place in the coming months. The wave of cuts has raised questions among some inside and outside the industry about Silicon Valley's commitment to providing extensive guardrails and user protections at a time when content moderation and misinformation remain challenging problems to solve. Some point to Musk's draconian cuts at Twitter as a pivot point for the industry. "Twitter making the first move provided cover for them," said Katie Paul, director of the online safety research group the Tech Transparency Project. (Twitter, which also cut much of its public relations team, did not respond to a request for comment.) To complicate matters, these cuts come as tech giants are rapidly rolling out transformative new technologies like artificial intelligence and virtual reality — both of which have sparked concerns about their potential impacts on users. "They're in a super, super tight race to the top for AI and I think they probably don't want teams slowing them down," said Jevin West, associate professor in the Information School at the University of Washington. But "it's an especially bad time to be getting rid of these teams when we're on the cusp of some pretty transformative, kind of scary technologies." "If you had the ability to go back and place these teams at the advent of social media, we'd probably be a little bit better off," West said. "We're at a similar moment right now with generative AI and these chatbots." Rethinking content moderation and ethical AI When Musk laid off thousands of Twitter employees following his takeover last fall, it included staffers focused on everything from security and site reliability to public policy and human rights issues. Since then, former employees, including ex-head of site integrity Yoel Roth — not to mention users and outside experts — have expressed concerns that Twitter's cuts could undermine its ability to handle content moderation. Months after Musk's initial moves, some former employees at Twitch, another popular social platform, are now worried about the impacts recent layoffs there could have on its ability to combat hate speech and harassment and to address emerging concerns from AI. One former Twitch employee affected by the layoffs and who previously worked on safety issues said the company had recently boosted its outsourcing capacity for addressing reports of violative content. "With that outsourcing, I feel like they had this comfort level that they could cut some of the trust and safety team, but Twitch is very unique," the former employee said. "It is truly live streaming, there is no post-production on uploads, so there is a ton of community engagement that needs to happen in real time." Such outsourced teams, as well as automated technology that helps platforms enforce their rules, also aren't as useful for proactive thinking about what a company's safety policies should be. "You're never going to stop having to be reactive to things, but we had started to really plan, move away from the reactive and really be much more proactive, and changing our policies out, making sure that they read better to our community," the employee told CNN, citing efforts like the launch of Twitch's online safety center and its Safety Advisory Council. Another former Twitch employee, who like the first spoke on condition of anonymity for fear of putting their severance at risk, told CNN that cutting back on responsible AI work, despite the fact that it wasn't a direct revenue driver, could be bad for business in the long run. "Problems are going to come up, especially now that AI is becoming part of the mainstream conversation," they said. "Safety, security and ethical issues are going to become more prevalent, so this is actually high time that companies should invest." Twitch declined to comment for this story beyond its blog post announcing layoffs. In that post, Twitch noted that users rely on the company to "give you the tools you need to build your communities, stream your passions safely, and make money doing what you love" and that "we take this responsibility incredibly seriously." Microsoft also raised some alarms earlier this month when it reportedly cut a key team focused on ethical AI product development as part of its mass layoffs. Former employees of the Microsoft team told The Verge that the Ethics and Society AI team was responsible for helping to translate the company's responsible AI principles for employees developing products. In a statement to CNN, Microsoft said the team "played a key role" in developing its responsible AI policies and practices, adding that its efforts have been ongoing since 2017. The company stressed that even with the cuts, "we have hundreds of people working on these issues across the company, including net new, dedicated responsible AI teams that have since been established and grown significantly during this time." An uncertain future at Meta Meta, maybe more than any other company, embodied the post-2016 shift toward greater safety measures and more thoughtful policies. It invested heavily in content moderation, public policy and an oversight board to weigh in on tricky content issues to address rising concerns about its platform. But Zuckerberg's recent announcement that Meta will undergo a second round of layoffs is raising questions about the fate of some of that work. Zuckerberg hinted that non-technical roles would take a hit and said non-engineering experts help "build better products, but with many new teams it takes intentional focus to make sure our company remains primarily technologists." Many of the cuts have yet to take place, meaning their impact, if any, may not be felt for months. And Zuckerberg said in his blog post announcing the layoffs that Meta "will make sure we continue to meet all our critical and legal obligations as we find ways to operate more efficiently." Still, "if it's claiming that they're going to focus on technology, it would be great if they would be more transparent about what teams they are letting go of," Paul said. "I suspect that there's a lack of transparency, because it's teams that deal with safety and security." Meta declined to comment for this story or answer questions about the details of its cuts beyond pointing CNN to Zuckerberg's blog post. Paul said Meta's emphasis on technology won't necessarily solve its ongoing issues. Research from the Tech Transparency Project last year found that Facebook's technology created dozens of pages for terrorist groups like ISIS and Al Qaeda. According to the organization's report, when a user listed a terrorist group on their profile or "checked in" to a terrorist group, a page for the group was automatically generated, although Facebook says it bans content from designated terrorist groups. "The technology that's supposed to be removing this content is actually creating it," Paul said. At the time the Tech Transparency Project report was published in September, Meta said in a comment that, "When these kinds of shell pages are auto-generated there is no owner or admin, and limited activity. As we said at the end of last year, we addressed an issue that auto-generated shell pages and we're continuing to review." 'Is this worth the investment?' In some cases, tech firms may feel emboldened to rethink investments in these teams by a lack of new laws. In the United States, lawmakers have imposed few new regulations, despite what West described as "a lot of political theater" in repeatedly calling out companies' safety failures. Tech leaders may also be grappling with the fact that even as they built up their trust and safety teams in recent years, their reputation problems haven't really abated. "All they keep getting is criticized," said Katie Harbath, former director of public policy at Facebook who now runs tech consulting firm Anchor Change. "I'm not saying they should get a pat on the back ... but there comes a point in time where I think Mark [Zuckerberg] and other CEOs are like, is this worth the investment?" While tech companies must balance their growth with the current economic conditions, Harbath said, "sometimes technologists think that they know the right things to do, they want to disrupt things, and aren't always as open to hearing from outside voices who aren't technologists." "You need that right balance to make sure you're not stifling innovation, but making sure that you're aware of the implications of what it is that you're building," she said. "We won't know until we see how things continue to operate moving forward, but my hope is that they at least continue to think about that."